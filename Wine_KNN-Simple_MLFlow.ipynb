{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, errno\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirs(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_wine(basename, dirname):\n",
    "    print(\"downloading wine dataset\")\n",
    "    data = datasets.load_wine(as_frame=True)\n",
    "    df = data.frame\n",
    "    targets = df.columns[~np.in1d(df.columns,data.feature_names)]\n",
    "    df[targets] = data.target_names[df[targets]]\n",
    "    makedirs(dirname)\n",
    "    with open(os.path.join(dirname, basename), \"w\") as fp:\n",
    "        df.to_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wine(basename=\"data.csv\", dirname=\"./datasets/wine\"):\n",
    "    if not os.path.exists( os.path.join(dirname, basename) ):\n",
    "        download_wine(basename, dirname)\n",
    "    with open(os.path.join(dirname, basename)) as fp:\n",
    "        dataset_df = pd.read_csv(fp, index_col=0)\n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_knn(n_neighbors=5, weights=\"uniform\"):    \n",
    "    # set the name of this run within the experiment\n",
    "    run_name = \"KNN({}, {}) - Simple MLFlow\".format(n_neighbors, weights)\n",
    "    mlflow.start_run(run_name=run_name)\n",
    "    \n",
    "    # load dataset\n",
    "    dataset_df = load_wine()\n",
    "    X = dataset_df[dataset_df.columns[:-1]]\n",
    "    y = dataset_df[dataset_df.columns[-1]]\n",
    "    \n",
    "    # create train/test splits\n",
    "    splits = train_test_split(X, y, test_size=0.25, random_state=0, shuffle=True, stratify=y)\n",
    "    X_train, X_test, y_train, y_test = splits\n",
    "    \n",
    "    # build classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # save model parameters\n",
    "    mlflow.log_param(\"n_neighbors\", n_neighbors)\n",
    "    mlflow.log_param(\"weights\",     weights)\n",
    "    \n",
    "    # evaluate performance\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    metrics = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "    precision, recall, fscore, support = metrics\n",
    "    print(\"{1:10.4f} {0:}\".format(\"Accuracy\",  accuracy))\n",
    "    print(\"{1:10.4f} {0:}\".format(\"Precision\", precision))\n",
    "    print(\"{1:10.4f} {0:}\".format(\"Recall\",    recall))\n",
    "    print(\"{1:10.4f} {0:}\".format(\"Fscore\",    fscore))\n",
    "\n",
    "    # save performance metrics\n",
    "    mlflow.log_metric(\"accuracy\",  accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\",    recall)\n",
    "    mlflow.log_metric(\"fscore\",    fscore)\n",
    "\n",
    "    # save the trained model\n",
    "    mlflow.sklearn.log_model(knn, \"model\")\n",
    "    \n",
    "    # always make sure to end the run\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the mlflow experiment\n",
    "mlflow.set_experiment(\"Wine (Simple-live)\")\n",
    "    \n",
    "print(\"KNN, n_neighbors=1\")\n",
    "train_knn(n_neighbors=1)\n",
    "print()\n",
    "\n",
    "print(\"KNN, n_neighbors=5, weights=uniform\")\n",
    "train_knn(n_neighbors=5, weights=\"uniform\")\n",
    "print()\n",
    "\n",
    "print(\"KNN, n_neighbors=5, weights=distance\")\n",
    "train_knn(n_neighbors=5, weights=\"distance\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the MLflow Python API logs runs locally to files in a `mlruns` directory wherever you ran your program. You can see the logged runs by running the `mlflow ui` command in the same directory as your code and then viewing the following webpage: http://localhost:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
